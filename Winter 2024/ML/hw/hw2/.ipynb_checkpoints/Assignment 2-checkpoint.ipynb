{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Linear Regression from Scratch (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some data points first, by the equation $y = x - 3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(100, size=100)/30 - 2\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "y = x + -3 + 0.3*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then visualize the data points we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Gradient of vanilla linear regression model (5 points)\n",
    "\n",
    "In the lecture, we learn that the cost function of a linear regression model can be expressed as **Equation 1**:\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{2 m} \\sum_{i}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$$ \n",
    "\n",
    "The gredient of it can be written as **Equation 2**:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta}=$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 Gradient of vanilla regularized regression model (5 points)\n",
    "\n",
    "After adding the L2 regularization term, the linear regression model can be expressed as **Equation 3**:\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{2 m} \\sum_{i}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}+\\frac{\\lambda}{2 m} \\sum_{j}^{n} (\\theta_{j})^{2}$$\n",
    "\n",
    "The gredient of it can be written as **Equation 4**:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta}=$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Implement the cost function of a regularized regression model (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please implement the cost function of a regularized regression model according to the above equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Implement the gradient of the cost function of a regularized regression model (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please implement the gradient of the cost function of a regularized regression model according to the above equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_linear_regression(X, y, alpha=0.01, lambda_value=1, epochs=30):\n",
    "    \"\"\"\n",
    "    :param x: feature matrix\n",
    "    :param y: target vector\n",
    "    :param alpha: learning rate (default:0.01)\n",
    "    :param lambda_value: lambda (default:1)\n",
    "    :param epochs: maximum number of iterations of the\n",
    "           linear regression algorithm for a single run (default=30)\n",
    "    :return: weights, list of the cost function changing overtime\n",
    "    \"\"\"\n",
    " \n",
    "    m = np.shape(X)[0]  # total number of samples\n",
    "    n = np.shape(X)[1]  # total number of features\n",
    " \n",
    "    X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "    W = np.random.randn(n + 1, )\n",
    " \n",
    "    # stores the updates on the cost function (loss function)\n",
    "    cost_history_list = []\n",
    " \n",
    "    # iterate until the maximum number of epochs\n",
    "    for current_iteration in np.arange(epochs):  # begin the process\n",
    " \n",
    "        # compute the dot product between our feature 'X' and weight 'W'\n",
    "        y_estimated = X.dot(W)\n",
    " \n",
    "        # calculate the difference between the actual and predicted value\n",
    "        error = y_estimated - y\n",
    "\n",
    "        \n",
    "############################################################################################################## \n",
    "####################################### Begin of Question 1.3 ################################################\n",
    "##############################################################################################################\n",
    "      \n",
    "        ##### Please write down your code here:####\n",
    "        \n",
    "        # calculate the cost (MSE) (Equation 1)\n",
    "        cost_without_regularization = \n",
    "        \n",
    "        \n",
    "        ##### Please write down your code here:####\n",
    "        \n",
    "        # regularization term\n",
    "        reg_term = \n",
    "        \n",
    "        \n",
    "        # calculate the cost (MSE) + regularization term (Equation 3)\n",
    "        cost_with_regularization = cost_without_regularization + reg_term\n",
    "        \n",
    "############################################################################################################## \n",
    "####################################### End of Question 1.3 ##################################################\n",
    "##############################################################################################################          \n",
    " \n",
    "    \n",
    "############################################################################################################## \n",
    "####################################### Begin of Question 1.4 ################################################\n",
    "############################################################################################################## \n",
    "        \n",
    "        ##### Please write down your code here:####\n",
    "        \n",
    "        # calculate the gradient of the cost function with regularization term (Equation )\n",
    "        gradient = \n",
    " \n",
    "\n",
    "        # Now we have to update our weights\n",
    "        W = W - alpha * gradient\n",
    "        \n",
    "############################################################################################################## \n",
    "####################################### End of Question 1.4 ##################################################\n",
    "##############################################################################################################          \n",
    " \n",
    " \n",
    "        # keep track the cost as it changes in each iteration\n",
    "        cost_history_list.append(cost_with_regularization)\n",
    "        \n",
    "    # Let's print out the cost\n",
    "    print(f\"Cost with regularization: {cost_with_regularization}\")\n",
    "    print(f\"Mean square error: {cost_without_regularization}\")\n",
    " \n",
    "    return W, cost_history_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to train your model. \n",
    "\n",
    "Hint: If you have the correct code written above, the cost should be $0.5181222986588751$ when $\\lambda = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lambda_list = [0, 10, 100, 1000, 10000]\n",
    "for lambda_ in lambda_list:\n",
    "    # calls regression function with different values of lambda\n",
    "    weight, _ = regularized_linear_regression(X, y, alpha=0.01,\n",
    "                                 lambda_value=lambda_, epochs=1000)\n",
    "    \n",
    "    fitted_line = np.dot(X, weight[1]) + weight[0]\n",
    "    plt.scatter(X, y, label='data points')\n",
    "    plt.plot(X, fitted_line, color='r', label='Fitted line')\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(f\"Regression (lambda : {lambda_})\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Analyze your results (10 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above figures, what's the best choice of $\\lambda$? \n",
    "\n",
    "Why the regressed line turns to be flat as we increase $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Getting familiar with PyTorch (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
