{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mltools as ml\n",
    "import time\n",
    "\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences Between Linear Classifier and Linear Regression\n",
    "We start with loading a dataset that was created for this discussion and talk a about the differences between linear regression and linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc2_data = np.genfromtxt('./lc2_data.txt', delimiter=None)\n",
    "X, Y = lc2_data[:, :-1], lc2_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "mask = Y == -1\n",
    "\n",
    "ax[0].scatter(X[mask, 0], X[mask, 1], s=120, color='blue', marker='s', alpha=0.75)\n",
    "ax[0].scatter(X[~mask, 0], X[~mask, 1], s=340, color='red', marker='*', alpha=0.75)\n",
    "\n",
    "ax[0].set_xticklabels(ax[0].get_xticks(), fontsize=25)\n",
    "ax[0].set_yticklabels(ax[0].get_yticks(), fontsize=25)\n",
    "\n",
    "ax[1].scatter(X[:, 0], X[:, 1], s=120, color='black', alpha=0.75)\n",
    "\n",
    "ax[1].set_xticklabels(ax[1].get_xticks(), fontsize=25)\n",
    "ax[1].set_yticklabels(ax[1].get_yticks(), fontsize=25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Classifier from the Ground Up\n",
    "In the rest of the discussion we will show how to code a classifier from the ground up. This will be extremely useful not only for your homework assignment but also for future reference, as implementations of machine learning algorithms use similar tools and conventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Algorithm\n",
    "As a simple example we will use the [Perceptron Algorithm](https://en.wikipedia.org/wiki/Perceptron). We will build each part seperately, showing how it works and end by wrapping it all up in a classifier class that can be used with the mltools library. \n",
    "\n",
    "We will use a 2 classes Perceptron with classes $\\{-1, 1\\}$. In the discussion you can also see how to use a binary classes $\\{0, 1\\}$ and in the wiki [page](https://en.wikipedia.org/wiki/Perceptron) you can see a generalization to multiple classes.\n",
    "\n",
    "For an illustration of the algorithm you can watch this YouTube [clip](https://www.youtube.com/watch?v=vGwemZhPlsA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundry and Classification\n",
    "The Perceptron uses a decision boundary $\\theta$ to compute a value for each point. Taking the sign of this value will then give us a class prediction.\n",
    "\n",
    "We'll start by computing the decision value for each point $x^j$: $$\\theta x^j$$\n",
    "\n",
    "As an example, let's choose $j=90$ (the 90th observation in our dataset) and let's define: $$\\theta = \\left[-6, 0.5, 1\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([-6., 0.5, 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the '.'s. This will make sure it's a float and not integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_const(X):\n",
    "    return np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "\n",
    "Xconst = add_const(X)\n",
    "x_j, y_j = Xconst[90], Y[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Value\n",
    "The first step in the preceptron is to compute the response value. It's comptued as the inner (dot) product $\\theta x^j$. The simple intuative way to do that is to use a for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_theta = 0\n",
    "for i in range(x_j.shape[0]):\n",
    "    x_theta += x_j[i] * theta[i]\n",
    "    \n",
    "print (x_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a VERY inefficient way to do compute a inner product. Luckily for us, numpy has the answer in the form of np.dot()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (np.dot(x_j, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Decision\n",
    "Now let's compute the decision classification $T[\\theta x^j]$. One option is to use the [np.sign](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sign.html) method. This is not a a good solution because np.sign(0) = 0.\n",
    "\n",
    "One solution is to check for 0s explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(vals):\n",
    "    \"\"\"Returns 1 if val >= 0 else -1\"\"\"\n",
    "    s = np.sign(vals)\n",
    "    try: # If vals is an array\n",
    "        s[s == 0] = 1\n",
    "    except: # If vals is a float\n",
    "        s = 1 if s == 0 else s\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict function\n",
    "So now with the the decision value and sign function we can write the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_j, theta):\n",
    "    \"\"\"Returns the class prediction of a single point x_j\"\"\"\n",
    "    return sign(np.dot(x_j, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (predict(x_j, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Prediction Error\n",
    "Using the predict function, we can now compute the prediction error: $$J^j = (y^j - \\hat{y}^j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_err(X, Y, theta):\n",
    "    \"\"\"Predicts that class for X and returns the error rate. \"\"\"\n",
    "    Yhat = predict(X, theta)\n",
    "    return np.mean(Yhat != Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pred_err(x_j, y_j, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Update\n",
    "Using the error we can now even do the update step in the learning algorithm: $$\\theta = \\theta + \\alpha * (y^j - \\hat{y}^j)x^j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.1\n",
    "y_hat_j = predict(x_j, theta)\n",
    "print (theta + a * (y_j - y_hat_j) * x_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train method\n",
    "Using everything we coded so far, we can fully create the train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, a=0.01, stop_tol=1e-8, max_iter=1000):\n",
    "    # Start by adding a const\n",
    "    Xconst = add_const(X)\n",
    "    \n",
    "    m, n = Xconst.shape\n",
    "    \n",
    "    # Initializing theta\n",
    "    theta = np.array([-6., 0.5, 1.])\n",
    "    \n",
    "    # The update loops\n",
    "    J_err = [np.inf]\n",
    "    \n",
    "    for i in range(1, max_iter + 1):             # Pass through the dataset max_iter times\n",
    "        \n",
    "        for j in range(m):                       # Loop through each observation\n",
    "            x_j, y_j = Xconst[j], Y[j]           # Get observation j\n",
    "            y_hat_j = predict(x_j, theta)        # Predict using the current theta\n",
    "            theta += a * (y_j - y_hat_j) * x_j   # Update theta\n",
    "\n",
    "        # Compute the error on the dataset after each pass\n",
    "        curr_err = pred_err(Xconst, Y, theta)\n",
    "        J_err.append(curr_err)\n",
    "\n",
    "        # Stop if the change in error is small\n",
    "        if np.abs(J_err[-2] - J_err[-1]) < stop_tol:\n",
    "            print ('Reached convergance after %d iterations. Prediction error is: %.3f' % (i, J_err[-1]))\n",
    "            break\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_trained = train(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Perceptron Classifier\n",
    "Now let's use all the code that we wrote and create a Python class Perceptron that can plug in to the mltools package.\n",
    "\n",
    "In order to do that, the Prceptron class has to inherit the object mltools.base.classifier\n",
    "\n",
    "In case you haven't looked at the actual code in the mltools, now will probably be the right time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools.base import classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to crete an object, we'll have to add self to all the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(classifier):\n",
    "    def __init__(self, theta=None):\n",
    "        self.theta = theta\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Retruns class prediction for either single point or multiple points. \"\"\"\n",
    "        # I'm addiing this stuff here so it could work with the plotClassify2D method.\n",
    "        Xconst = np.atleast_2d(X)\n",
    "        \n",
    "        # Making sure it has the const, if not adding it.\n",
    "        if Xconst.shape[1] == self.theta.shape[0] - 1:\n",
    "            Xconst = add_const(Xconst)\n",
    "        \n",
    "        return self.sign(np.dot(Xconst, self.theta))\n",
    "                \n",
    "    def sign(self, vals):\n",
    "        \"\"\"A sign version with breaking 0's as +1. \"\"\"\n",
    "        return np.sign(vals + 1e-200)\n",
    "    \n",
    "    def pred_err(self, X, Y):\n",
    "        Yhat = self.predict(X)\n",
    "        return np.mean(Yhat != Y)\n",
    "    \n",
    "    def train(self, X, Y, a=0.02, stop_tol=1e-8, max_iter=1000):\n",
    "        # Start by adding a const\n",
    "        Xconst = add_const(X)\n",
    "\n",
    "        m, n = Xconst.shape\n",
    "        \n",
    "        # Making sure Theta is inititialized.\n",
    "        if self.theta is None:\n",
    "            self.theta = np.random.random(n)\n",
    "\n",
    "        # The update loops\n",
    "        J_err = [np.inf]\n",
    "        for i in range(1, max_iter + 1):\n",
    "            for j in range(m):\n",
    "                x_j, y_j = Xconst[j], Y[j]\n",
    "                y_hat_j = self.predict(x_j)\n",
    "                self.theta += a * (y_j - y_hat_j) * x_j\n",
    "                \n",
    "            curr_err = self.pred_err(Xconst, Y)\n",
    "            J_err.append(curr_err)\n",
    "\n",
    "            if np.abs(J_err[-2] - J_err[-1]) < stop_tol:\n",
    "                print ('Reached convergance after %d iterations. Prediction error is: %.3f' % (i, J_err[-1]))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model, training and plotting predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create the model with some initialized theta and plot the decision bounderies. For the plotting we can use the mltools plotClassify2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron()\n",
    "model.theta = np.array([-6., 0.5, 1])\n",
    "\n",
    "ml.plotClassify2D(model, X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's actually train the model and plot the new decision boundery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.theta = np.array([-6., 0.5, 1])\n",
    "model.train(X, Y)\n",
    "ml.plotClassify2D(model, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the best classifier!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
