{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to assist you in playing around with some classifiers, though most of the code is already in the homework assignment writeup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "from __future__ import division # For python 2.*\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mltools as ml\n",
    "\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "X = np.genfromtxt(\"data/X_train.txt\", delimiter=None)\n",
    "Y = np.genfromtxt(\"data/Y_train.txt\", delimiter=None)\n",
    "\n",
    "# The test data\n",
    "Xte = np.genfromtxt(\"data/X_test.txt\", delimiter=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All your work should be done on the training data set. To be able to make educated decisions on which classifier you're going to use, you should split it into train and validation data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xva, Ytr, Yva = ml.splitData(X, Y)\n",
    "Xtr, Ytr = ml.shuffleData(Xtr, Ytr)\n",
    "\n",
    "# Taking a subsample of the data so that trains faster.  You should train on whole data for homework.\n",
    "Xt, Yt = Xtr[:4000], Ytr[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000, 14)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 14)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decision tree classifier has minLeaf and maxDepth parameters.\n",
    "learner = ml.dtree.treeClassify(Xt, Yt, minLeaf=25, maxDepth=15)\n",
    "\n",
    "# Prediction\n",
    "probs = learner.predictSoft(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictSoft method returns an $M \\times C$ table in which for each point you have the proability of each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75      , 0.25      ],\n",
       "       [0.90384615, 0.09615385],\n",
       "       [0.92424242, 0.07575758],\n",
       "       ...,\n",
       "       [0.94827586, 0.05172414],\n",
       "       [0.7175    , 0.2825    ],\n",
       "       [0.98214286, 0.01785714]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the AUC for both the training and validation data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Train AUC: 0.8066\n",
      " Validation AUC: 0.6209\n"
     ]
    }
   ],
   "source": [
    "# ROC is a probability curve and AUC represents degree or measure of separability\n",
    "# AUC is the area under the ROC curve\n",
    "print(\"{0:>15}: {1:.4f}\".format(\"Train AUC\", learner.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format(\"Validation AUC\", learner.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with different parameters to see how AUC changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing decision tree\n",
    "\n",
    "Funny enough, whoever wrote the decision tree classifier provided a printing mechanism. However, it only works up to depth 2, so not very useful for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  if x[0] < 237.245000:\n",
      "    if x[2] < 242.215000:\n",
      "      Predict [0.45227996 0.54772004]\n",
      "    else:\n",
      "      Predict [0.69883041 0.30116959]\n",
      "  else:\n",
      "    if x[0] < 249.030000:\n",
      "      Predict [0.69539337 0.30460663]\n",
      "    else:\n",
      "      Predict [0.79384134 0.20615866]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner = ml.dtree.treeClassify()\n",
    "learner.train(Xt, Yt, maxDepth=2)\n",
    "print(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 14) (200000, 14)\n"
     ]
    }
   ],
   "source": [
    "# Scaling the data\n",
    "XtrP, params = ml.rescale(Xt)\n",
    "XteP, _ = ml.rescale(Xte, params)\n",
    "\n",
    "print(XtrP.shape, XteP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7794684  0.2205316 ]\n",
      " [0.56946355 0.43053645]\n",
      " [0.81647372 0.18352628]\n",
      " [0.79368576 0.20631424]\n",
      " [0.72786685 0.27213315]]\n"
     ]
    }
   ],
   "source": [
    "## Linear models:\n",
    "learner = ml.linearC.linearClassify()\n",
    "learner.train(XtrP, Yt, initStep=0.5, stopTol=1e-6, stopIter=100)\n",
    "\n",
    "probs = learner.predictSoft(XteP)\n",
    "print(probs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the AUC IS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Train AUC: 0.6472\n",
      " Validation AUC: 0.5763\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:>15}: {1:.4f}\".format(\"Train AUC\", learner.auc(XtrP, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format(\"Validation AUC\", learner.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why we're using a validation data set. We can see already that for THIS specific configuration the decision tree is much better. It is very likely that it'll be better on the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ml.nnet.nnetClassify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we construct the classifier, we can define the sizes of its layers and initialize their values with \"init_weights\".\n",
    "\n",
    "Definition of nn.init_weights:\n",
    "\n",
    "        nn.init_weights(self, sizes, init, X, Y)\n",
    "\n",
    "From the method description: sizes = [Ninput, N1, N2, ... , Noutput], where Ninput = # of input features, and Nouput = # classes\n",
    "\n",
    "Training the model using gradient descent, we can track the surrogate loss (here, MSE loss on the output vector, compared to a 1-of-K representation of the class), as well as the 0/1 classification loss (error rate):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(Yt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first layer matrix: 14*5, activation function, second layer matrix: 5*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first layer matrix: 14*5, activation function,, second layer matrix: 5*3, activation function,, third layer: 3*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1 : Jsur = 0.47327088546279183, J01 = 0.338\n",
      "it 2 : Jsur = 0.458980643301179, J01 = 0.338\n",
      "it 4 : Jsur = 0.4352169633533841, J01 = 0.338\n",
      "it 8 : Jsur = 0.43531926047066105, J01 = 0.338\n",
      "it 16 : Jsur = 0.43416758865144195, J01 = 0.338\n",
      "it 32 : Jsur = 0.4342372845514982, J01 = 0.338\n"
     ]
    }
   ],
   "source": [
    "# Need to specify the right number of input and output layers.\n",
    "nn.init_weights([Xt.shape[1], 5, len(np.unique(Yt))], \"random\", Xt, Yt)\n",
    "nn.train(\n",
    "    Xt, Yt, stopTol=1e-8, stepsize=0.25, stopIter=50\n",
    ")  # Really small stopIter so it will stop fast :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Train AUC: 0.5993\n",
      " Validation AUC: 0.5929\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:>15}: {1:.4f}\".format(\"Train AUC\", nn.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format(\"Validation AUC\", nn.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC results are bad because we just used a lame configuration of the NN.\n",
    "One example is the option to change the activation function. This is the function that is in the inner layers. By default the code comes with the tanh, but the logistic (sigmoid) is also coded in and you can just specify it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harryxiong24/Code/Study/grad-code-collection/Winter 2024/ML/lab/Lab6/mltools/nnet.py:218: RuntimeWarning: overflow encountered in exp\n",
      "  self.Sig = lambda z: twod(1 / (1 + np.exp(-z)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1 : Jsur = 0.447668013819423, J01 = 0.338\n",
      "it 2 : Jsur = 0.44747146909571944, J01 = 0.338\n",
      "it 4 : Jsur = 0.44747349960993926, J01 = 0.338\n",
      "it 8 : Jsur = 0.4474867698659233, J01 = 0.338\n",
      "it 16 : Jsur = 0.4474868530202253, J01 = 0.338\n",
      "it 32 : Jsur = 0.4474739262918074, J01 = 0.338\n",
      "it 64 : Jsur = 0.44748047965190857, J01 = 0.338\n",
      "      Train AUC: 0.5002\n",
      " Validation AUC: 0.5002\n"
     ]
    }
   ],
   "source": [
    "nn.setActivation(\"logistic\")\n",
    "\n",
    "nn.train(Xt, Yt, stopTol=1e-8, stepsize=0.25, stopIter=100)\n",
    "print(\"{0:>15}: {1:.4f}\".format(\"Train AUC\", nn.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format(\"Validation AUC\", nn.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your own activation function\n",
    "\n",
    "Not suprisingly, you can also provide a custom activation function. Note that for the last layer you will probably always want the sigmoid function, so only change the inner layers ones.\n",
    "\n",
    "The function definition is this:\n",
    "\n",
    "    setActivation(self, method, sig=None, d_sig=None, sig_0=None, d_sig_0=None)\n",
    "\n",
    "You can call it with method='custom' and then specify both sig and d_sig. (the '0' ones are for the last layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a dummy activation method (f(x) = x)\n",
    "sig = lambda z: np.atleast_2d(z)\n",
    "dsig = lambda z: np.atleast_2d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1 : Jsur = nan, J01 = 0.338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harryxiong24/Code/Study/grad-code-collection/Winter 2024/ML/lab/Lab6/mltools/nnet.py:48: RuntimeWarning: overflow encountered in exp\n",
      "  self.Sig0 = lambda Z: 1.0/(1.0 + np.exp(-Z))   # final layer nonlinearity & derivative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 2 : Jsur = nan, J01 = 0.338\n",
      "it 4 : Jsur = nan, J01 = 0.338\n",
      "it 8 : Jsur = nan, J01 = 0.338\n",
      "it 16 : Jsur = nan, J01 = 0.338\n",
      "it 32 : Jsur = nan, J01 = 0.338\n",
      "it 64 : Jsur = nan, J01 = 0.338\n",
      "      Train AUC: 0.5198\n",
      " Validation AUC: 0.4999\n"
     ]
    }
   ],
   "source": [
    "nn = ml.nnet.nnetClassify()\n",
    "nn.init_weights([Xt.shape[1], 5, len(np.unique(Yt))], \"random\", Xt, Yt)\n",
    "\n",
    "nn.setActivation(\"custom\", sig, dsig)\n",
    "\n",
    "nn.train(Xt, Yt, stopTol=1e-8, stepsize=0.25, stopIter=100)\n",
    "print(\"{0:>15}: {1:.4f}\".format(\"Train AUC\", nn.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format(\"Validation AUC\", nn.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train AUC: 0.5198 <br>\n",
    "Validation AUC: 0.4999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "We've learn that one way of guessing how well we're doing with different model parameters is to plot the train and validation errors as a function of that paramter (e.g, k in the KNN of degree in the linear classifier and regression).\n",
    "\n",
    "Now it seems like there could be more parameters involved? One example is the degree and the regularizer value (see. HW assignment for more examples).\n",
    "\n",
    "When it's two features you can use heatmaps. The X-axis and Y-axis represent the parameters and the \"heat\" is the validation/train error as a \"third\" dimension\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a dummy function to show that. Let's assume we have two parameters p1 and p2 and the prediction accuracy is p1 + p2 (yup, that stupid). In the HW assignment it's actually the auc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.arange(5)\n",
    "p2 = np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = np.zeros([p1.shape[0], p2.shape[0]])\n",
    "for i in range(p1.shape[0]):\n",
    "    for j in range(p2.shape[0]):\n",
    "        auc[i][j] = p1[i] + p2[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 3., 4.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [2., 3., 4., 5., 6.],\n",
       "       [3., 4., 5., 6., 7.],\n",
       "       [4., 5., 6., 7., 8.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "cax = ax.matshow(auc)\n",
    "f.colorbar(cax)\n",
    "\n",
    "ax.set_xticks(p1)\n",
    "ax.set_xticklabels([\"%d\" % p for p in p1])\n",
    "\n",
    "ax.set_yticks(p2)\n",
    "ax.set_yticklabels([\"%d\" % p for p in p2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> For homework: <br> <br>\n",
    "\n",
    "x and y will be hyperparameters of the model <br>\n",
    "f will be some performance metric (error, AUC etc.) </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
